# 第二阶段：Web 前后端看板架构方案设计

**目标**：替代 Power BI，使用自定义前后端架构，实现看板数据的**自动持续更新**，并支持基于**时间维度的 KPI 筛选与趋势分析**。

---

## 1. 核心架构选型 (轻量级本地化部署)

鉴于目前项目主体使用 Python 开发，且部署环境主要在内网（基于现有爬虫要求），推荐选型如下：

*   **后端开发**：`FastAPI` (Python)
    *   *理由*：原生支持异步，性能极佳，自带 Swagger 接口文档，且能无缝衔接现有的 Python 爬虫和分析脚本(Pandas处理)。
*   **数据库**：`SQLite` 升级到 `PostgreSQL` (或暂时保留 SQLite)
    *   *理由*：要实现“时间筛选和趋势分析”，必须放弃过去“覆盖写入最新的_latest.csv”的模式。需要一个关系型数据库来追加存储每天/每小时的**快照数据 (Snapshot)** 和**历史明细流水**。考虑到本业务数据量（每天几万条明细），单机 PostgreSQL 或者简单的 SQLite 就能支撑。
*   **自动化调度**：`APScheduler` (集成在 FastAPI 中) 或 OS级 `Crontab`
    *   *理由*：全量爬虫较慢（尤其是 NWMS 发料明细），需要安排在非工作时间（如凌晨）自动后台增量拉取。工作时间内则可设置高频拉取库存状态。
*   **前端应用**：`React` (Vite) + `ECharts` + `Vanilla CSS`
    *   *理由*：按最佳实践建立高级感的暗黑模式UI，配合 ECharts 构建生产看板数据图表（柱状图、折线趋势图、动态散点图）。

---

## 2. 数据层设计改造（支持时间维度）

从现有的 CSV 静态分析，转向**时间序列化数据库存储**是核心。

### 2.1 引入数据批次概念 (Batch / Snapshot)

每次爬虫和分析脚本完整运行后，生成一个全局唯一的 `batch_id`，并记录当前的 `timestamp`。所有分析结果均挂载在该 `batch_id` 下。

### 2.2 核心数据表设计

1.  **`kpi_history` (KPI 趋势主表)**
    *   存储每次 Batch 运行时的全局 KPI汇总。
    *   字段：`batch_id`, `timestamp` (用于时间筛选), `alert_group_count` (退料预警组数), `high_risk_count` (高风险组数), `over_issue_lines` (超发行数), `avg_aging_hours` (平均库龄)。
2.  **`alert_report_snapshots` (离场审计-退料预警快照表)**
    *   字段继承原 `alert_report.csv`，外加 `batch_id` 和 `timestamp`。
    *   以便前端查询：“在某一天/某一段时间内，各产线的退料预警组数趋势变化”。
3.  **`issue_audit_snapshots` (进场审计-超发预警快照表)**
    *   包含所有超发记录及其所属批次的时间。
4.  **`material_inventory_log` (库存流水明细 - 可选进阶)**
    *   用于追溯单个条码在某一天是否在线边仓，便于精确追溯特定物料在过去一周的状态变化。

---

## 3. 后端 API 接口设计

FastAPI 提供如下核心 RESTful 接口供前端调用：

*   **`GET /api/kpi/summary`**
    *   *参数*：`start_time`, `end_time` (用于时间筛选)
    *   *返回*：指定时间段内（或最新一次）的 KPI 聚合数据。
*   **`GET /api/kpi/trend`**
    *   *参数*：`start_time`, `end_time`, `interval` (如按天、按小时)
    *   *返回*：用于绘制 ECharts 折线图的时间序列数据（退料预警组数的历史起伏）。
*   **`GET /api/alerts/top10`**
    *   *参数*：可选的时间戳，获取某一时段的严重退料清单 Top 10。
*   **`GET /api/alerts/detail`**
    *   *参数*：时间范围、工单号、产线等筛选条件。
    *   *返回*：带分页的退料/超发明细列表（替换原始 PBI 的 Drill-through 功能）。
*   **`POST /api/task/trigger`** (预留)
    *   允许管理员查看到数据不刷新时，在页面上手动触发后台强制拉取最新数据。

---

## 4. 前端交互设计与功能

*   **全局时间切片器 (Time Slicer)**
    *   提供 `[实时最新] [过去24小时] [过去7天] [过去30天] [自定义时间区间]`。所有图表和表格与该组件联动。
*   **实时自动刷新**
    *   页面每隔 5 分钟自动调用最新 `summary` API，大屏展示无需手动刷新。
*   **高级设计美学 (UI)**
    *   **暗色主题 (Dark Mode)**：主打高级深色背景搭配高饱和度的质感图表（ECharts），减少长时间观看的刺眼感。
    *   **视觉警报**：发生严重滞留/超发时，核心卡片增加微动效（如呼吸灯闪烁提示、红/黄/橙状态色）。

---

## 5. 实施路径 (Action Plan)

1.  **第一步：数据库建模与数据流解耦**
    *   创建 SQLite 数据库引擎。
    *   新增 `src/db/sync.py` 用于触发 `build_report.py` 纯函数获取返回结果，利用 `SQLAlchemy` 将数据带上当前时间戳 (`timestamp` 和 `batch_id`) **追加写入 (INSERT)** SQLite 的快照表中。原基于 CSV 的第一期分析逻辑不受影响，实现双模解耦。
2.  **第二步：搭建 FastAPI 后端与自动化调度器**
    *   起一个干净的 FastAPI 服务。
    *   引入 `APScheduler` 定时触发爬虫和重跑分析，定时写库。
    *   编写支持时间范围聚合的 API (如使用 Pandas 加载 DB 数据或者直接写 SQL 聚合)。
3.  **第三步：构建 React + Vite 大屏看板**
    *   设计看板 UI 并接入 API，实现高级感的暗黑主题数据展示。

> ℹ️ **说明**：上述步骤一~三期的构建目前均已完全落地开发完成并全流程走通，进入常态化运行阶段。
